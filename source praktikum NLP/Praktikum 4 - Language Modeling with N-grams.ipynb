{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import alpino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ngrams(alpino.words(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De',)\n",
      "('verzekeringsmaatschappijen',)\n",
      "('verhelen',)\n",
      "('niet',)\n",
      "('dat',)\n"
     ]
    }
   ],
   "source": [
    "for i, gram in enumerate(unigrams):\n",
    "    if i < 5:\n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgrams = ngrams(alpino.words(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De', 'verzekeringsmaatschappijen', 'verhelen', 'niet')\n",
      "('verzekeringsmaatschappijen', 'verhelen', 'niet', 'dat')\n",
      "('verhelen', 'niet', 'dat', 'ook')\n",
      "('niet', 'dat', 'ook', 'de')\n",
      "('dat', 'ook', 'de', 'rentegrondslag')\n"
     ]
    }
   ],
   "source": [
    "for i,gram in enumerate(quadgrams):\n",
    "    if i < 5:\n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'),\n",
       " ('arthur', ':'),\n",
       " ('#', '1'),\n",
       " (\"'\", 't'),\n",
       " ('villager', '#'),\n",
       " ('#', '2'),\n",
       " (']', '['),\n",
       " ('1', ':'),\n",
       " ('oh', ','),\n",
       " ('black', 'knight')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import webtext\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "tokens = [t.lower() for t in webtext.words('grail.txt')]\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "words.nbest(BigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away'),\n",
       " ('french', 'guard'),\n",
       " ('cartoon', 'character')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "set = set(stopwords.words('english'))\n",
    "stops_filter = lambda w: len(w) < 3 or w in set\n",
    "tokens = [t.lower() for t in webtext.words('grail.txt')]\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "words.apply_word_filter(stops_filter)\n",
    "words.nbest(BigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'Never'),\n",
       " ('Hardwork', 'is'),\n",
       " ('Never', 'give'),\n",
       " ('give', 'up'),\n",
       " ('is', 'the'),\n",
       " ('key', 'to'),\n",
       " ('success', '.'),\n",
       " ('the', 'key'),\n",
       " ('to', 'success'),\n",
       " ('up', '!')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=\"Hardwork is the key to success.Never give up!\"\n",
    "word = nltk.wordpunct_tokenize(text1)\n",
    "finder = BigramCollocationFinder.from_words(word)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "value = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigrams_tokens=ngrams(alpino.words(),2)\n",
    "\n",
    "for o, gram in enumerate(bigrams_tokens):\n",
    "    if i < 5:\n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De', 'verzekeringsmaatschappijen', 'verhelen')\n",
      "('verzekeringsmaatschappijen', 'verhelen', 'niet')\n",
      "('verhelen', 'niet', 'dat')\n",
      "('niet', 'dat', 'ook')\n",
      "('dat', 'ook', 'de')\n"
     ]
    }
   ],
   "source": [
    "trigrams_tokens=ngrams(alpino.words(),3)\n",
    "\n",
    "for i, gram in enumerate(trigrams_tokens):\n",
    "    if i < 5:\n",
    "        print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello', 'how', 'are', 'you') 1\n",
      "('how', 'are', 'you', 'doing') 1\n",
      "('are', 'you', 'doing', '?') 1\n",
      "('you', 'doing', '?', 'i') 1\n",
      "('doing', '?', 'i', 'hope') 1\n",
      "('?', 'i', 'hope', 'you') 1\n",
      "('i', 'hope', 'you', 'find') 1\n",
      "('hope', 'you', 'find', 'the') 1\n",
      "('you', 'find', 'the', 'book') 1\n",
      "('find', 'the', 'book', 'interesting') 1\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello how are you doing? i hope you find the book interesting\"\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "fourgrams = nltk.collocations.QuadgramCollocationFinder.from_words(tokens)\n",
    "for fourgram, freq in fourgrams.ngram_fd.items():\n",
    "    print(fourgram,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello,please', 'read', 'the', 'book', 'thoroughly,If')\n",
      "('read', 'the', 'book', 'thoroughly,If', 'you')\n",
      "('the', 'book', 'thoroughly,If', 'you', 'hae')\n",
      "('book', 'thoroughly,If', 'you', 'hae', 'anyquaries,then')\n",
      "('thoroughly,If', 'you', 'hae', 'anyquaries,then', \"don't\")\n",
      "('you', 'hae', 'anyquaries,then', \"don't\", 'hesitate')\n",
      "('hae', 'anyquaries,then', \"don't\", 'hesitate', 'to')\n",
      "('anyquaries,then', \"don't\", 'hesitate', 'to', 'ask.There')\n",
      "(\"don't\", 'hesitate', 'to', 'ask.There', 'is')\n",
      "('hesitate', 'to', 'ask.There', 'is', 'no')\n",
      "('to', 'ask.There', 'is', 'no', 'shortcut')\n",
      "('ask.There', 'is', 'no', 'shortcut', 'to')\n",
      "('is', 'no', 'shortcut', 'to', 'success.')\n"
     ]
    }
   ],
   "source": [
    "sent = \"Hello,please read the book thoroughly,If you hae any\\\n",
    "quaries,then don't hesitate to ask.There is no shortcut to success.\"\n",
    "n=5\n",
    "fivegrams = ngrams(sent.split(),n)\n",
    "for grams in fivegrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "filename = 'G:/SEMESTER 7/NLP\\WEEK 4/PRAKTIKUM/story2.txt'\n",
    "with io.open(filename) as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "    tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                    for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "n=3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['the']]['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['the', 'king']]['heard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30434782608695654"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('king', 'the'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('heard', 'the king'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "\n",
    "filename = 'G:/SEMESTER 7/NLP\\WEEK 4/PRAKTIKUM/story2.txt'\n",
    "with io.open(filename) as fin:\n",
    "    text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 'heaven'),\n",
       " ('golden', 'swan'),\n",
       " ('good', 'deeds'),\n",
       " ('old', 'man'),\n",
       " ('the', 'king'),\n",
       " ('come', 'to'),\n",
       " ('do', 'good'),\n",
       " ('i', 'am'),\n",
       " ('heaven', '.\"'),\n",
       " ('.', 'but')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "tokens = [t.lower() for t in webtext.words('G:/SEMESTER 7/NLP\\WEEK 4/PRAKTIKUM/story2.txt')]\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "words.nbest(BigramAssocMeasures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('come', 'to', 'heaven', '.\"'),\n",
       " ('will', 'come', 'to', 'heaven'),\n",
       " ('way', 'to', 'heaven', '.\"'),\n",
       " ('must', 'come', 'to', 'heaven'),\n",
       " ('to', 'heaven', '.\"', 'surprised'),\n",
       " ('\\x93', 'do', 'good', 'deeds'),\n",
       " ('only', 'way', 'to', 'heaven'),\n",
       " ('to', 'heaven', '.', 'after'),\n",
       " ('the', 'way', 'to', 'heaven'),\n",
       " ('to', 'heaven', '.\"', 'it')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.metrics.association import QuadgramAssocMeasures\n",
    "\n",
    "tokens = [t.lower() for t in webtext.words('G:/SEMESTER 7/NLP\\WEEK 4/PRAKTIKUM/story2.txt')]\n",
    "words = QuadgramCollocationFinder.from_words(tokens)\n",
    "words.nbest(QuadgramAssocMeasures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.menerapkan evaluator language model dengan perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def unigram(tokens):    \n",
    "    model = collections.defaultdict(lambda: 0.01)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model [f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380.79207920791964\n",
      "95.91022443890246\n"
     ]
    }
   ],
   "source": [
    "testset1 = \"Long\"\n",
    "testset2 = \"people\"\n",
    "\n",
    "model = unigram(tokens)\n",
    "print(perplexity(testset1, model))\n",
    "print(perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. menerapkan smoothing pada model MLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "corpus = \"I am the Golden Swan. If you want to capture me, you must come to heaven.\".split()\n",
    "sentence = \"I am the Golden Swan.\".split()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "[cfd[a][b] for (a,b) in nltk.bigrams(sentence)]\n",
    "[cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. back-off dan interpolation pada model MLE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import NgramTagger\n",
    "\n",
    "trains = brown.tagged_sents(categories=\"news\")\n",
    "tagger = None        \n",
    "for n in range(1,4): \n",
    "    tagger = NgramTagger(n, trains, backoff=tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b0687e741966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'story2.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getText' is not defined"
     ]
    }
   ],
   "source": [
    "tagger.tag(getText('story2.txt').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Long/JJ)\n",
      "  time/NN\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  there/EX\n",
      "  lived/VBD\n",
      "  a/AT\n",
      "  King/NN-TL\n",
      "  ./.)\n",
      "(S\n",
      "  He/PPS\n",
      "  was/BEDZ\n",
      "  lazy/JJ\n",
      "  and/CC\n",
      "  liked/VBD\n",
      "  all/ABN\n",
      "  the/AT\n",
      "  comforts/NNS\n",
      "  of/IN\n",
      "  life/NN\n",
      "  ./.)\n",
      "(S\n",
      "  He/PPS\n",
      "  never/RB\n",
      "  carried/VBN\n",
      "  out/RP\n",
      "  his/PP$\n",
      "  duties/NNS\n",
      "  as/CS\n",
      "  a/AT\n",
      "  King/NN-TL\n",
      "  ./.)\n",
      "(S\n",
      "  “/NN\n",
      "  Our/PP$\n",
      "  King/NN-TL\n",
      "  does/DOZ\n",
      "  not/*\n",
      "  take/VB\n",
      "  care/NN\n",
      "  of/IN\n",
      "  our/PP$\n",
      "  needs/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  He/PPS\n",
      "  also/RB\n",
      "  ignores/NNS\n",
      "  the/AT\n",
      "  affairs/NNS\n",
      "  of/IN\n",
      "  his/PP$\n",
      "  kingdom/NN\n",
      "  ./.\n",
      "  ''/'')\n",
      "(S The/AT people/NNS complained/VBD ./.)\n",
      "(S\n",
      "  One/CD\n",
      "  day/NN\n",
      "  ,/,\n",
      "  the/AT\n",
      "  (ORGANIZATION King/NN-TL)\n",
      "  went/VBD\n",
      "  into/IN\n",
      "  the/AT\n",
      "  forest/NN\n",
      "  to/TO\n",
      "  hunt/NN\n",
      "  ./.)\n",
      "(S\n",
      "  After/IN\n",
      "  having/HVG\n",
      "  wandered/VBD\n",
      "  for/IN\n",
      "  quite/QL\n",
      "  sometime/RB\n",
      "  ,/,\n",
      "  he/PPS\n",
      "  became/VBD\n",
      "  thirsty/NN\n",
      "  ./.)\n",
      "(S To/TO his/PP$ relief/NN ,/, he/PPS spotted/VBD a/AT lake/NN ./.)\n",
      "(S\n",
      "  As/CS\n",
      "  he/PPS\n",
      "  was/BEDZ\n",
      "  drinking/VBG\n",
      "  water/NN\n",
      "  ,/,\n",
      "  he/PPS\n",
      "  suddenly/RB\n",
      "  saw/VBD\n",
      "  a/AT\n",
      "  golden/JJ\n",
      "  swan/NN\n",
      "  come/VB\n",
      "  out/RP\n",
      "  of/IN\n",
      "  the/AT\n",
      "  lake/NN\n",
      "  and/CC\n",
      "  perch/NN\n",
      "  on/IN\n",
      "  a/AT\n",
      "  stone/NN\n",
      "  ./.)\n",
      "(S “/NN Oh/NN !/.)\n",
      "(S A/AT golden/JJ swan/NN ./.)\n",
      "(S\n",
      "  I/PPSS\n",
      "  must/MD\n",
      "  capture/VB\n",
      "  it/PPO\n",
      "  ,/,\n",
      "  ''/''\n",
      "  thought/VBD\n",
      "  the/AT\n",
      "  King/NN-TL\n",
      "  ./.)\n",
      "(S\n",
      "  But/CC\n",
      "  as/CS\n",
      "  soon/RB\n",
      "  as/CS\n",
      "  he/PPS\n",
      "  held/VBN\n",
      "  his/PP$\n",
      "  bow/NN\n",
      "  up/IN\n",
      "  ,/,\n",
      "  the/AT\n",
      "  swan/NN\n",
      "  disappeared/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  And/CC\n",
      "  the/AT\n",
      "  King/NN-TL\n",
      "  heard/VBN\n",
      "  a/AT\n",
      "  voice/NN\n",
      "  ,/,\n",
      "  “/NN\n",
      "  I/PPSS\n",
      "  am/BEM\n",
      "  the/AT\n",
      "  (ORGANIZATION Golden/JJ-TL Swan/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  If/CS\n",
      "  you/PPSS\n",
      "  want/VB\n",
      "  to/TO\n",
      "  capture/VB\n",
      "  me/PPO\n",
      "  ,/,\n",
      "  you/PPSS\n",
      "  must/MD\n",
      "  come/VB\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ./.\n",
      "  ''/'')\n",
      "(S\n",
      "  Surprised/VBN\n",
      "  ,/,\n",
      "  the/AT\n",
      "  (ORGANIZATION King/NN-TL)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  “/NN\n",
      "  Please/NN\n",
      "  show/NN\n",
      "  me/PPO\n",
      "  the/AT\n",
      "  way/NN\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ./.\n",
      "  ''/'')\n",
      "(S\n",
      "  “/NN\n",
      "  Do/DO\n",
      "  good/JJ\n",
      "  deeds/NNS\n",
      "  ,/,\n",
      "  serve/VB\n",
      "  your/PP$\n",
      "  people/NNS\n",
      "  and/CC\n",
      "  the/AT\n",
      "  messenger/NN\n",
      "  from/IN\n",
      "  heaven/NN\n",
      "  would/MD\n",
      "  come/VB\n",
      "  to/TO\n",
      "  fetch/NN\n",
      "  you/PPSS\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  replied/VBD\n",
      "  the/AT\n",
      "  voice/NN\n",
      "  ./.)\n",
      "(S\n",
      "  The/AT\n",
      "  selfish/JJ\n",
      "  King/NN-TL\n",
      "  ,/,\n",
      "  eager/JJ\n",
      "  to/IN\n",
      "  capture/VB\n",
      "  the/AT\n",
      "  (GPE Swan/NN)\n",
      "  ,/,\n",
      "  tried/VBD\n",
      "  doing/VBG\n",
      "  some/DTI\n",
      "  good/JJ\n",
      "  deeds/NNS\n",
      "  in/IN\n",
      "  his/PP$\n",
      "  (PERSON Kingdom/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  “/NN\n",
      "  Now/RB\n",
      "  ,/,\n",
      "  I/PPSS\n",
      "  suppose/NN\n",
      "  a/AT\n",
      "  messenger/NN\n",
      "  will/MD\n",
      "  come/VB\n",
      "  to/TO\n",
      "  take/VB\n",
      "  me/PPO\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  he/PPS\n",
      "  thought/VBD\n",
      "  ./.)\n",
      "(S But/CC ,/, no/AT messenger/NN came/VBD ./.)\n",
      "(S\n",
      "  The/AT\n",
      "  King/NN-TL\n",
      "  then/RB\n",
      "  disguised/VBD\n",
      "  himself/PPL\n",
      "  and/CC\n",
      "  went/VBD\n",
      "  out/RP\n",
      "  into/IN\n",
      "  the/AT\n",
      "  street/NN\n",
      "  ./.)\n",
      "(S There/EX he/PPS tried/VBD helping/VBG an/AT old/JJ man/NN ./.)\n",
      "(S\n",
      "  But/CC\n",
      "  the/AT\n",
      "  old/JJ\n",
      "  man/NN\n",
      "  became/VBD\n",
      "  angry/JJ\n",
      "  and/CC\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  “/NN\n",
      "  You/PPSS\n",
      "  need/VB\n",
      "  not/*\n",
      "  try/VB\n",
      "  to/TO\n",
      "  help/VB\n",
      "  ./.)\n",
      "(S\n",
      "  I/PPSS\n",
      "  am/BEM\n",
      "  in/IN\n",
      "  this/DT\n",
      "  miserable/JJ\n",
      "  state/NN\n",
      "  because/CS\n",
      "  of/IN\n",
      "  out/IN\n",
      "  selfish/JJ\n",
      "  King/NN-TL\n",
      "  ./.)\n",
      "(S\n",
      "  He/PPS\n",
      "  has/HVZ\n",
      "  done/VBN\n",
      "  nothing/PN\n",
      "  for/IN\n",
      "  his/PP$\n",
      "  people/NNS\n",
      "  ./.\n",
      "  ''/'')\n",
      "(S\n",
      "  Suddenly/RB\n",
      "  ,/,\n",
      "  the/AT\n",
      "  King/NN-TL\n",
      "  heard/VBN\n",
      "  the/AT\n",
      "  golden/JJ\n",
      "  swan/NN\n",
      "  ’/NN\n",
      "  s/NNS\n",
      "  voice/NN\n",
      "  ,/,\n",
      "  “/NN\n",
      "  Do/DO\n",
      "  good/JJ\n",
      "  deeds/NNS\n",
      "  and/CC\n",
      "  you/PPSS\n",
      "  will/MD\n",
      "  come/VB\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ./.\n",
      "  ''/'')\n",
      "(S\n",
      "  It/PPS\n",
      "  dawned/VBD\n",
      "  on/IN\n",
      "  the/AT\n",
      "  King/NN-TL\n",
      "  that/CS\n",
      "  by/IN\n",
      "  doing/VBG\n",
      "  selfish/JJ\n",
      "  acts/NNS\n",
      "  ,/,\n",
      "  he/PPS\n",
      "  will/MD\n",
      "  not/*\n",
      "  go/VB\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ./.)\n",
      "(S\n",
      "  He/PPS\n",
      "  realized/VBD\n",
      "  that/CS\n",
      "  his/PP$\n",
      "  people/NNS\n",
      "  needed/VBD\n",
      "  him/PPO\n",
      "  and/CC\n",
      "  carrying/VBG\n",
      "  out/RP\n",
      "  his/PP$\n",
      "  duties/NNS\n",
      "  was/BEDZ\n",
      "  the/AT\n",
      "  only/AP\n",
      "  way/NN\n",
      "  to/TO\n",
      "  heaven/NN\n",
      "  ./.)\n",
      "(S\n",
      "  After/IN\n",
      "  that/DT\n",
      "  day/NN\n",
      "  he/PPS\n",
      "  became/VBD\n",
      "  a/AT\n",
      "  responsible/JJ\n",
      "  King/NN-TL\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import sys\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "def tri_gram():\n",
    "    b_t_sents=brown.tagged_sents(categories='news')\n",
    "    default_tagger = nltk.RegexpTagger(\n",
    "        [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "         (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "         (r'.*able$', 'JJ'),                # adjectives\n",
    "         (r'.*ness$', 'NN'),                # nouns formed from adjectives  \n",
    "         (r'.*ly$', 'RB'),                  # adverbs\n",
    "         (r'.*s$', 'NNS'),                  # plural nouns  \n",
    "         (r'.*ing$', 'VBG'),                # gerunds   \n",
    "         (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "         (r'.*', 'NN')                      # nouns (default)\n",
    "        ])\n",
    "    u_gram_tag=nltk.UnigramTagger(b_t_sents,backoff=default_tagger) \n",
    "    b_gram_tag=nltk.BigramTagger(b_t_sents,backoff=u_gram_tag)\n",
    "    t_gram_tag=nltk.TrigramTagger(b_t_sents,backoff=b_gram_tag)\n",
    "\n",
    "    ##pos of given text\n",
    "    f_read=open(\"G:/SEMESTER 7/NLP\\WEEK 4/PRAKTIKUM/story2.txt\",'r')\n",
    "    given_text=f_read.read();\n",
    "    segmented_lines=nltk.sent_tokenize(given_text) \n",
    "    for text in segmented_lines:\n",
    "        words=word_tokenize(text)\n",
    "        sent = t_gram_tag.tag(words)\n",
    "        print(ne_chunk(sent))\n",
    "tri_gram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
